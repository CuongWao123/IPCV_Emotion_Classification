{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "448dfe62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in d:\\hcmut\\computer vision\\ipcv_project\\.venv\\lib\\site-packages (2.2.6)\n",
      "Requirement already satisfied: matplotlib in d:\\hcmut\\computer vision\\ipcv_project\\.venv\\lib\\site-packages (3.10.6)\n",
      "Requirement already satisfied: opencv-python in d:\\hcmut\\computer vision\\ipcv_project\\.venv\\lib\\site-packages (4.12.0.88)\n",
      "Requirement already satisfied: scikit-learn in d:\\hcmut\\computer vision\\ipcv_project\\.venv\\lib\\site-packages (1.7.2)\n",
      "Requirement already satisfied: scikit-image in d:\\hcmut\\computer vision\\ipcv_project\\.venv\\lib\\site-packages (0.25.2)\n",
      "Requirement already satisfied: tensorflow in d:\\hcmut\\computer vision\\ipcv_project\\.venv\\lib\\site-packages (2.20.0)\n",
      "Requirement already satisfied: keras in d:\\hcmut\\computer vision\\ipcv_project\\.venv\\lib\\site-packages (3.11.3)\n",
      "Requirement already satisfied: pandas in d:\\hcmut\\computer vision\\ipcv_project\\.venv\\lib\\site-packages (2.3.3)\n",
      "Requirement already satisfied: seaborn in d:\\hcmut\\computer vision\\ipcv_project\\.venv\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: tqdm in d:\\hcmut\\computer vision\\ipcv_project\\.venv\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: mtcnn in d:\\hcmut\\computer vision\\ipcv_project\\.venv\\lib\\site-packages (1.0.0)\n",
      "Requirement already satisfied: lz4 in d:\\hcmut\\computer vision\\ipcv_project\\.venv\\lib\\site-packages (4.4.4)\n",
      "Requirement already satisfied: facenet_pytorch in d:\\hcmut\\computer vision\\ipcv_project\\.venv\\lib\\site-packages (2.5.3)\n",
      "Requirement already satisfied: xgboost in d:\\hcmut\\computer vision\\ipcv_project\\.venv\\lib\\site-packages (3.0.5)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in d:\\hcmut\\computer vision\\ipcv_project\\.venv\\lib\\site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in d:\\hcmut\\computer vision\\ipcv_project\\.venv\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in d:\\hcmut\\computer vision\\ipcv_project\\.venv\\lib\\site-packages (from matplotlib) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in d:\\hcmut\\computer vision\\ipcv_project\\.venv\\lib\\site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\hcmut\\computer vision\\ipcv_project\\.venv\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in d:\\hcmut\\computer vision\\ipcv_project\\.venv\\lib\\site-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in d:\\hcmut\\computer vision\\ipcv_project\\.venv\\lib\\site-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in d:\\hcmut\\computer vision\\ipcv_project\\.venv\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: scipy>=1.8.0 in d:\\hcmut\\computer vision\\ipcv_project\\.venv\\lib\\site-packages (from scikit-learn) (1.16.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in d:\\hcmut\\computer vision\\ipcv_project\\.venv\\lib\\site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in d:\\hcmut\\computer vision\\ipcv_project\\.venv\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in d:\\hcmut\\computer vision\\ipcv_project\\.venv\\lib\\site-packages (from scikit-image) (3.5)\n",
      "Requirement already satisfied: imageio!=2.35.0,>=2.33 in d:\\hcmut\\computer vision\\ipcv_project\\.venv\\lib\\site-packages (from scikit-image) (2.37.0)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in d:\\hcmut\\computer vision\\ipcv_project\\.venv\\lib\\site-packages (from scikit-image) (2025.9.30)\n",
      "Requirement already satisfied: lazy-loader>=0.4 in d:\\hcmut\\computer vision\\ipcv_project\\.venv\\lib\\site-packages (from scikit-image) (0.4)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in d:\\hcmut\\computer vision\\ipcv_project\\.venv\\lib\\site-packages (from tensorflow) (2.3.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in d:\\hcmut\\computer vision\\ipcv_project\\.venv\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in d:\\hcmut\\computer vision\\ipcv_project\\.venv\\lib\\site-packages (from tensorflow) (25.9.23)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in d:\\hcmut\\computer vision\\ipcv_project\\.venv\\lib\\site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google_pasta>=0.1.1 in d:\\hcmut\\computer vision\\ipcv_project\\.venv\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in d:\\hcmut\\computer vision\\ipcv_project\\.venv\\lib\\site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt_einsum>=2.3.2 in d:\\hcmut\\computer vision\\ipcv_project\\.venv\\lib\\site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: protobuf>=5.28.0 in d:\\hcmut\\computer vision\\ipcv_project\\.venv\\lib\\site-packages (from tensorflow) (6.32.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in d:\\hcmut\\computer vision\\ipcv_project\\.venv\\lib\\site-packages (from tensorflow) (2.32.5)\n",
      "Requirement already satisfied: setuptools in d:\\hcmut\\computer vision\\ipcv_project\\.venv\\lib\\site-packages (from tensorflow) (80.9.0)\n",
      "Requirement already satisfied: six>=1.12.0 in d:\\hcmut\\computer vision\\ipcv_project\\.venv\\lib\\site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in d:\\hcmut\\computer vision\\ipcv_project\\.venv\\lib\\site-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: typing_extensions>=3.6.6 in d:\\hcmut\\computer vision\\ipcv_project\\.venv\\lib\\site-packages (from tensorflow) (4.15.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in d:\\hcmut\\computer vision\\ipcv_project\\.venv\\lib\\site-packages (from tensorflow) (1.17.3)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in d:\\hcmut\\computer vision\\ipcv_project\\.venv\\lib\\site-packages (from tensorflow) (1.75.1)\n",
      "Requirement already satisfied: tensorboard~=2.20.0 in d:\\hcmut\\computer vision\\ipcv_project\\.venv\\lib\\site-packages (from tensorflow) (2.20.0)\n",
      "Requirement already satisfied: h5py>=3.11.0 in d:\\hcmut\\computer vision\\ipcv_project\\.venv\\lib\\site-packages (from tensorflow) (3.14.0)\n",
      "Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in d:\\hcmut\\computer vision\\ipcv_project\\.venv\\lib\\site-packages (from tensorflow) (0.5.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\hcmut\\computer vision\\ipcv_project\\.venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\hcmut\\computer vision\\ipcv_project\\.venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\hcmut\\computer vision\\ipcv_project\\.venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\hcmut\\computer vision\\ipcv_project\\.venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2025.8.3)\n",
      "Requirement already satisfied: markdown>=2.6.8 in d:\\hcmut\\computer vision\\ipcv_project\\.venv\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (3.9)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in d:\\hcmut\\computer vision\\ipcv_project\\.venv\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in d:\\hcmut\\computer vision\\ipcv_project\\.venv\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: rich in d:\\hcmut\\computer vision\\ipcv_project\\.venv\\lib\\site-packages (from keras) (14.1.0)\n",
      "Requirement already satisfied: namex in d:\\hcmut\\computer vision\\ipcv_project\\.venv\\lib\\site-packages (from keras) (0.1.0)\n",
      "Requirement already satisfied: optree in d:\\hcmut\\computer vision\\ipcv_project\\.venv\\lib\\site-packages (from keras) (0.17.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\hcmut\\computer vision\\ipcv_project\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\hcmut\\computer vision\\ipcv_project\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: colorama in d:\\hcmut\\computer vision\\ipcv_project\\.venv\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: torchvision in d:\\hcmut\\computer vision\\ipcv_project\\.venv\\lib\\site-packages (from facenet_pytorch) (0.23.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in d:\\hcmut\\computer vision\\ipcv_project\\.venv\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in d:\\hcmut\\computer vision\\ipcv_project\\.venv\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in d:\\hcmut\\computer vision\\ipcv_project\\.venv\\lib\\site-packages (from rich->keras) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in d:\\hcmut\\computer vision\\ipcv_project\\.venv\\lib\\site-packages (from rich->keras) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in d:\\hcmut\\computer vision\\ipcv_project\\.venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
      "Requirement already satisfied: torch==2.8.0 in d:\\hcmut\\computer vision\\ipcv_project\\.venv\\lib\\site-packages (from torchvision->facenet_pytorch) (2.8.0)\n",
      "Requirement already satisfied: filelock in d:\\hcmut\\computer vision\\ipcv_project\\.venv\\lib\\site-packages (from torch==2.8.0->torchvision->facenet_pytorch) (3.19.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in d:\\hcmut\\computer vision\\ipcv_project\\.venv\\lib\\site-packages (from torch==2.8.0->torchvision->facenet_pytorch) (1.14.0)\n",
      "Requirement already satisfied: jinja2 in d:\\hcmut\\computer vision\\ipcv_project\\.venv\\lib\\site-packages (from torch==2.8.0->torchvision->facenet_pytorch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in d:\\hcmut\\computer vision\\ipcv_project\\.venv\\lib\\site-packages (from torch==2.8.0->torchvision->facenet_pytorch) (2025.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\hcmut\\computer vision\\ipcv_project\\.venv\\lib\\site-packages (from sympy>=1.13.3->torch==2.8.0->torchvision->facenet_pytorch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install numpy matplotlib opencv-python scikit-learn scikit-image tensorflow keras pandas seaborn tqdm mtcnn lz4 facenet_pytorch xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b516eda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\HCMUT\\Computer Vision\\IPCV_Project\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from utils import canny, load_fer2013_data\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Linear models\n",
    "from sklearn.linear_model import LogisticRegression, Ridge, Lasso\n",
    "\n",
    "# Tree-based models\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "\n",
    "# SVM models\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "\n",
    "# Naive Bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
    "from skimage.feature import hog , local_binary_pattern \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "from facenet_pytorch import MTCNN\n",
    "import lz4 \n",
    "\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09169ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, emotion_labels = load_fer2013_data('data/train')\n",
    "print(f\"Dataset shape: {X.shape}\")\n",
    "print(f\"Labels shape: {y.shape}\")\n",
    "print(f\"Emotion labels: {emotion_labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e931c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  logistic regression , SVM , Decision Tree , Random Forest \n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"Linear SVM\": LinearSVC(max_iter=1000) , \n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5952507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed evaluation and visualization\n",
    "\n",
    "def evaluate_models(model_results, y_test, emotion_labels):\n",
    "    \"\"\"\n",
    "    Detailed evaluation of all trained models\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"DETAILED MODEL EVALUATION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Extract results\n",
    "    model_names = list(model_results.keys())\n",
    "    accuracies = [model_results[name]['accuracy'] for name in model_names]\n",
    "    \n",
    "    # 1. Accuracy comparison plot\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    plt.subplot(1, 3, 1)\n",
    "    colors = ['blue', 'green', 'red', 'orange']\n",
    "    bars = plt.bar(model_names, accuracies, color=colors, alpha=0.7)\n",
    "    plt.title('Model Accuracy Comparison')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.ylim(0, 1)\n",
    "    \n",
    "    # Add accuracy values on bars\n",
    "    for bar, acc in zip(bars, accuracies):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "                f'{acc:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    # 2. Precision, Recall, F1 comparison\n",
    "    plt.subplot(1, 3, 2)\n",
    "    metrics_data = []\n",
    "    for name in model_names:\n",
    "        report = model_results[name]['classification_report']\n",
    "        metrics_data.append([\n",
    "            report['macro avg']['precision'],\n",
    "            report['macro avg']['recall'],\n",
    "            report['macro avg']['f1-score']\n",
    "        ])\n",
    "    \n",
    "    metrics_data = np.array(metrics_data)\n",
    "    x = np.arange(len(model_names))\n",
    "    width = 0.25\n",
    "    \n",
    "    plt.bar(x - width, metrics_data[:, 0], width, label='Precision', alpha=0.7)\n",
    "    plt.bar(x, metrics_data[:, 1], width, label='Recall', alpha=0.7)\n",
    "    plt.bar(x + width, metrics_data[:, 2], width, label='F1-Score', alpha=0.7)\n",
    "    \n",
    "    plt.xlabel('Models')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('Macro-averaged Metrics')\n",
    "    plt.xticks(x, model_names, rotation=45)\n",
    "    plt.legend()\n",
    "    plt.ylim(0, 1)\n",
    "    \n",
    "    # 3. Summary table\n",
    "    plt.subplot(1, 3, 3)\n",
    "    summary_data = []\n",
    "    for name in model_names:\n",
    "        report = model_results[name]['classification_report']\n",
    "        summary_data.append([\n",
    "            name[:10] + '...' if len(name) > 10 else name,\n",
    "            f\"{model_results[name]['accuracy']:.3f}\",\n",
    "            f\"{report['macro avg']['f1-score']:.3f}\"\n",
    "        ])\n",
    "    \n",
    "    plt.axis('tight')\n",
    "    plt.axis('off')\n",
    "    table = plt.table(cellText=summary_data,\n",
    "                     colLabels=['Model', 'Accuracy', 'F1-Score'],\n",
    "                     cellLoc='center',\n",
    "                     loc='center')\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(9)\n",
    "    table.scale(1.2, 1.5)\n",
    "    plt.title('Results Summary')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print detailed classification reports\n",
    "    for model_name, result in model_results.items():\n",
    "        print(f\"\\n{model_name.upper()} - Classification Report:\")\n",
    "        print(\"-\" * 50)\n",
    "        print(classification_report(y_test, result['predictions'], target_names=emotion_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7326ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model and create final summary\n",
    "\n",
    "def save_best_model(model_results, emotion_labels):\n",
    "    \"\"\"\n",
    "    Save the best performing model and create final summary\n",
    "    \"\"\"\n",
    "    import joblib\n",
    "    \n",
    "    # Find best model\n",
    "    best_model_name = max(model_results.keys(), key=lambda k: model_results[k]['accuracy'])\n",
    "    best_model = model_results[best_model_name]['model']\n",
    "    best_accuracy = model_results[best_model_name]['accuracy']\n",
    "    \n",
    "    # Create summary DataFrame\n",
    "    summary_data = []\n",
    "    for model_name, result in model_results.items():\n",
    "        summary_data.append({\n",
    "            'Model': model_name,\n",
    "            'Accuracy': result['accuracy'],\n",
    "            'Precision_macro': result['classification_report']['macro avg']['precision'],\n",
    "            'Recall_macro': result['classification_report']['macro avg']['recall'],\n",
    "            'F1_macro': result['classification_report']['macro avg']['f1-score']\n",
    "        })\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    summary_df = summary_df.sort_values('Accuracy', ascending=False)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"FINAL BASELINE RESULTS SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    print(summary_df.round(4).to_string(index=False))\n",
    "    \n",
    "    print(f\"\\nBest Model: {best_model_name}\")\n",
    "    print(f\"Best Accuracy: {best_accuracy:.4f}\")\n",
    "    \n",
    "    # Package everything for saving\n",
    "    model_package = {\n",
    "        'best_model': best_model,\n",
    "        'best_model_name': best_model_name,\n",
    "        'emotion_labels': emotion_labels,\n",
    "        'all_results': model_results,\n",
    "        'summary': summary_df,\n",
    "        'target_size': (48, 48)\n",
    "    }\n",
    "    \n",
    "    # Save to file\n",
    "    joblib.dump(model_package, 'data/baseline_emotion_models.pkl')\n",
    "    print(f\"\\nAll models and preprocessing components saved to 'baseline_emotion_models.pkl'\")\n",
    "    \n",
    "    return summary_df, best_model_name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ecd2e3",
   "metadata": {},
   "source": [
    "# Pipeline Experiment A: HOG + SVM-RBF + ROI\n",
    "\n",
    "## 1. Dataset & Split\n",
    "- Dataset: FER2013 / RAF-DB (7 lá»›p cáº£m xÃºc).\n",
    "- Input: áº£nh khuÃ´n máº·t Ä‘Ã£ crop.\n",
    "- Chia táº­p: Train 70% â€“ Val 15% â€“ Test 15% (stratified).\n",
    "---\n",
    "## 2. Meadian filer + CLAHE \n",
    "---\n",
    "## 3. Resize + Normalize\n",
    "---\n",
    "## 4. Feature Extraction \n",
    "  - HOG \n",
    "  - LBP \n",
    "  - SIFT\n",
    "---\n",
    "## 5. Model Training\n",
    "- Classifier: **SVM-RBF** , **Random Forest** , **Logistic Regression** , \n",
    "---\n",
    "## 6. Evaluation\n",
    "- **Chá»‰ sá»‘ chÃ­nh**: Macro-F1.\n",
    "- Phá»¥: Accuracy, per-class F1, Confusion matrix.\n",
    "- PhÃ¢n tÃ­ch lá»—i: fear â†” surprise, disgust â†” sad/angry.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930f22f8",
   "metadata": {},
   "source": [
    "## Preprocessing \n",
    "- CLAHE \n",
    "- Meadian Filter \n",
    "- GF, ER , MR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b31346",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_transformation_comparison(X_original, X_transformed, transformation_name, \n",
    "                                 emotion_labels, y, n_samples=8, figsize=(20, 8)):\n",
    "    \"\"\"\n",
    "    Universal function to plot comparison between original and transformed images\n",
    "    \n",
    "    Parameters:\n",
    "    - X_original: Original images\n",
    "    - X_transformed: Transformed images  \n",
    "    - transformation_name: Name of the transformation (e.g., \"CLAHE\", \"Median Filter\")\n",
    "    - emotion_labels: List of emotion labels\n",
    "    - y: Label array\n",
    "    - n_samples: Number of samples to display\n",
    "    - figsize: Figure size\n",
    "    \"\"\"\n",
    "    # Randomly select some indices for visualization\n",
    "    indices = np.random.choice(len(X_original), n_samples, replace=False)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, n_samples, figsize=figsize)\n",
    "    \n",
    "    for i, idx in enumerate(indices):\n",
    "        # Original image\n",
    "        orig_img = X_original[idx]\n",
    "        \n",
    "        # Handle different data ranges for display\n",
    "        if orig_img.max() <= 1.0:\n",
    "            orig_vmin, orig_vmax = 0, 1\n",
    "        else:\n",
    "            orig_vmin, orig_vmax = 0, 255\n",
    "            \n",
    "        axes[0, i].imshow(orig_img, cmap='gray', vmin=orig_vmin, vmax=orig_vmax)\n",
    "        axes[0, i].set_title(f'Original\\n{emotion_labels[y[idx]]}', fontsize=10)\n",
    "        axes[0, i].axis('off')\n",
    "        \n",
    "        # Transformed image\n",
    "        trans_img = X_transformed[idx]\n",
    "        \n",
    "        # Handle different data ranges for display\n",
    "        if trans_img.max() <= 1.0:\n",
    "            trans_vmin, trans_vmax = 0, 1\n",
    "        else:\n",
    "            trans_vmin, trans_vmax = 0, 255\n",
    "            \n",
    "        axes[1, i].imshow(trans_img, cmap='gray', vmin=trans_vmin, vmax=trans_vmax)\n",
    "        axes[1, i].set_title(f'{transformation_name}\\n{emotion_labels[y[idx]]}', fontsize=10)\n",
    "        axes[1, i].axis('off')\n",
    "    \n",
    "    plt.suptitle(f'Original vs {transformation_name} Enhanced Images', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f5ed2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_median_filtered = np.array([cv2.medianBlur(img, 3) for img in X])  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a14fa8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_transformation_comparison(X, X_median_filtered, \"Median Filter\", emotion_labels, y, n_samples=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b3dddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply clahe to enhance image contrast\n",
    "def apply_clahe(X, clip_limit=2.0, tile_grid_size=(8, 8)):\n",
    "    \"\"\"\n",
    "    Apply CLAHE (Contrast Limited Adaptive Histogram Equalization) to enhance image contrast\n",
    "    \"\"\"\n",
    "    clahe = cv2.createCLAHE(clipLimit=clip_limit, tileGridSize=tile_grid_size)\n",
    "    enhanced_images = []\n",
    "    \n",
    "    print(\"Applying CLAHE to images...\")\n",
    "    for img in tqdm(X):\n",
    "        # Convert to uint8 if needed\n",
    "        if img.dtype != np.uint8:\n",
    "            # Normalize to 0-255 range if needed\n",
    "            if img.max() <= 1.0:  # If image is in [0,1] range\n",
    "                img_uint8 = (img * 255).astype(np.uint8)\n",
    "            else:  # If image is already in [0,255] range but wrong dtype\n",
    "                img_uint8 = np.clip(img, 0, 255).astype(np.uint8)\n",
    "        else:\n",
    "            img_uint8 = img\n",
    "        \n",
    "        # Apply CLAHE\n",
    "        img_enhanced = clahe.apply(img_uint8)\n",
    "        enhanced_images.append(img_enhanced)\n",
    "    \n",
    "    return np.array(enhanced_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf787bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_median_clahe = apply_clahe(X_median_filtered, clip_limit=2.0, tile_grid_size=(8, 8)) \n",
    "plot_transformation_comparison(X, x_median_clahe, \"Median Filter -> CLAHE\", emotion_labels, y, n_samples=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e93ea05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Side-by-side detailed comparison with zoom\n",
    "\n",
    "def detailed_clahe_comparison(X_original, X_clahe, emotion_labels, y, sample_idx=None):\n",
    "    \"\"\"\n",
    "    Detailed side-by-side comparison with zoomed regions\n",
    "    \"\"\"\n",
    "    if sample_idx is None:\n",
    "        sample_idx = np.random.randint(0, len(X_original))\n",
    "    \n",
    "    orig_img = X_original[sample_idx]\n",
    "    clahe_img = X_clahe[sample_idx]\n",
    "    emotion = emotion_labels[y[sample_idx]]\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    \n",
    "    # Full images\n",
    "    axes[0, 0].imshow(orig_img, cmap='gray')\n",
    "    axes[0, 0].set_title(f'Original - {emotion}', fontsize=14)\n",
    "    axes[0, 0].axis('off')\n",
    "    \n",
    "    axes[0, 1].imshow(clahe_img, cmap='gray')\n",
    "    axes[0, 1].set_title(f'CLAHE Enhanced - {emotion}', fontsize=14)\n",
    "    axes[0, 1].axis('off')\n",
    "    \n",
    "    # Difference image\n",
    "    diff_img = np.abs(clahe_img.astype(float) - orig_img.astype(float))\n",
    "    axes[0, 2].imshow(diff_img, cmap='hot')\n",
    "    axes[0, 2].set_title('Absolute Difference', fontsize=14)\n",
    "    axes[0, 2].axis('off')\n",
    "    \n",
    "    # Histograms\n",
    "    axes[1, 0].hist(orig_img.flatten(), bins=50, alpha=0.7, color='blue', density=True)\n",
    "    axes[1, 0].set_title('Original Histogram')\n",
    "    axes[1, 0].set_xlabel('Pixel Intensity')\n",
    "    axes[1, 0].set_ylabel('Density')\n",
    "    \n",
    "    axes[1, 1].hist(clahe_img.flatten(), bins=50, alpha=0.7, color='red', density=True)\n",
    "    axes[1, 1].set_title('CLAHE Histogram')\n",
    "    axes[1, 1].set_xlabel('Pixel Intensity')\n",
    "    axes[1, 1].set_ylabel('Density')\n",
    "    \n",
    "    # Overlay histograms\n",
    "    axes[1, 2].hist(orig_img.flatten(), bins=50, alpha=0.5, color='blue', density=True, label='Original')\n",
    "    axes[1, 2].hist(clahe_img.flatten(), bins=50, alpha=0.5, color='red', density=True, label='CLAHE')\n",
    "    axes[1, 2].set_title('Histogram Overlay')\n",
    "    axes[1, 2].set_xlabel('Pixel Intensity')\n",
    "    axes[1, 2].set_ylabel('Density')\n",
    "    axes[1, 2].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print statistics for this specific image\n",
    "    print(f\"Image Statistics for sample {sample_idx} ({emotion}):\")\n",
    "    print(f\"Original  - Mean: {np.mean(orig_img):.2f}, Std: {np.std(orig_img):.2f}\")\n",
    "    print(f\"CLAHE     - Mean: {np.mean(clahe_img):.2f}, Std: {np.std(clahe_img):.2f}\")\n",
    "    print(f\"Contrast improvement: {np.std(clahe_img) - np.std(orig_img):.2f}\")\n",
    "\n",
    "# Show detailed comparison for a random sample\n",
    "detailed_clahe_comparison(X, x_median_clahe, emotion_labels=emotion_labels, y=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0064664",
   "metadata": {},
   "source": [
    "## Resize and normalize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1331513a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resize to (64, 64) and normalize to [0, 1]\n",
    "x_resized = np.array([cv2.resize(img, (64, 64)) for img in x_median_clahe])\n",
    "x_preprocessed = x_resized.astype('float32') / 255.0\n",
    "x_preprocessed = x_preprocessed.reshape(-1, 64, 64, 1)  # Add channel dimension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4a7a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_transformation_comparison(X , x_preprocessed, \"Preprocessed (64x64 + Norm)\", emotion_labels, y, n_samples=8 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4850a131",
   "metadata": {},
   "source": [
    "## HOG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791530c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.feature import hog\n",
    "x_hog = []\n",
    "for img in tqdm(x_preprocessed):\n",
    "    hog_features = hog(\n",
    "        img.squeeze(),\n",
    "        pixels_per_cell=(8, 8),\n",
    "        cells_per_block=(2, 2),\n",
    "        visualize=False    )\n",
    "    x_hog.append(hog_features)\n",
    "x_hog = np.array(x_hog)\n",
    "print(f\"HOG feature shape: {x_hog.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4016bd89",
   "metadata": {},
   "source": [
    "## LBP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea23c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lbp \n",
    "def lbp_grid_hist(img, P=8, R=1, grid=(8,8), method='uniform'):\n",
    "    H, W = img.shape\n",
    "    gy, gx = grid\n",
    "    lbp = local_binary_pattern(img, P=P, R=R, method=method)\n",
    "    \n",
    "    n_bins = P + 2\n",
    "    cell_h, cell_w = H // gy, W // gx\n",
    "\n",
    "    feats = []\n",
    "    for i in range(gy):\n",
    "        for j in range(gx):\n",
    "            y0, y1 = i*cell_h, (i+1)*cell_h if i<gy-1 else H\n",
    "            x0, x1 = j*cell_w, (j+1)*cell_w if j<gx-1 else W\n",
    "            cell = lbp[y0:y1, x0:x1].ravel()\n",
    "            hist, _ = np.histogram(cell, bins=np.arange(0, n_bins+1), range=(0, n_bins))\n",
    "            hist = hist.astype(np.float32)\n",
    "            hist /= (hist.sum() + 1e-8)\n",
    "            feats.append(hist)\n",
    "    return np.concatenate(feats, axis=0)  # (gy*gx*n_bins,)\n",
    "\n",
    "x_lbp = []\n",
    "for img in tqdm(x_preprocessed):\n",
    "    lbp_features = lbp_grid_hist(img.squeeze(), P=8, R=1, grid=(8,8), method='uniform')\n",
    "    x_lbp.append(lbp_features)\n",
    "\n",
    "x_lbp = np.array(x_lbp)\n",
    "print(f\"LBP feature shape: {x_lbp.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17e9d78",
   "metadata": {},
   "source": [
    "## SIFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb572d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_sift = []\n",
    "sift = cv2.SIFT_create()\n",
    "for img in tqdm(x_preprocessed):\n",
    "    keypoints, descriptors = sift.detectAndCompute((img.squeeze() * 255).astype('uint8'), None)\n",
    "    if descriptors is not None:\n",
    "        sift_feature = descriptors.flatten()\n",
    "        if sift_feature.shape[0] < 128 * 10:  # Pad to fixed size\n",
    "            sift_feature = np.pad(sift_feature, (0, 128 * 10 - sift_feature.shape[0]), 'constant')\n",
    "        else:\n",
    "            sift_feature = sift_feature[:128 * 10]  # Truncate if too long\n",
    "    else:\n",
    "        sift_feature = np.zeros(128 * 10)  \n",
    "    x_sift.append(sift_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39556a1b",
   "metadata": {},
   "source": [
    "## Combine all feature "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7adca74",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.concatenate([x_hog, x_lbp, x_sift], axis=1)\n",
    "print(f\"Combined feature shape: {X_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13dc5dbe",
   "metadata": {},
   "source": [
    "## Prepare for Model \n",
    "\n",
    "- Standard scaler \n",
    "- PCA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0258df30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard scaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaler  = scaler.fit_transform(X_train)\n",
    "# PCA \n",
    "pca = PCA(n_components=0.95, random_state=42)  # Retain 95% variance\n",
    "X_train_pca = pca.fit_transform(X_train_scaler)\n",
    "print(f\"PCA reduced shape: {X_train_pca.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033caffe",
   "metadata": {},
   "source": [
    "## Prepare test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830f8664",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test , y_test , labels = load_fer2013_data('data/test') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209098bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_test_data(X_test, scaler, pca, target_size=(64, 64)):\n",
    "    \"\"\"\n",
    "    Preprocess test data using the same pipeline as training data\n",
    "    \n",
    "    Parameters:\n",
    "    - X_test: Raw test images\n",
    "    - scaler: Fitted StandardScaler from training\n",
    "    - pca: Fitted PCA from training\n",
    "    - target_size: Target image size (default: (64, 64))\n",
    "    \n",
    "    Returns:\n",
    "    - X_test_processed: Preprocessed test data ready for model prediction\n",
    "    - preprocessing_info: Dictionary containing intermediate results for analysis\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"ðŸ”„ Processing test data through the same pipeline...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Store intermediate results for analysis\n",
    "    preprocessing_info = {}\n",
    "    \n",
    "    # Step 1: Median Filter\n",
    "    print(\"Step 1/8: Applying Median Filter...\")\n",
    "    X_test_median = np.array([cv2.medianBlur(img, 3) for img in tqdm(X_test)])\n",
    "    preprocessing_info['median_filtered'] = X_test_median\n",
    "    \n",
    "    # Step 2: CLAHE Enhancement\n",
    "    print(\"Step 2/8: Applying CLAHE Enhancement...\")\n",
    "    X_test_clahe = apply_clahe(X_test_median, clip_limit=2.0, tile_grid_size=(8, 8))\n",
    "    preprocessing_info['clahe_enhanced'] = X_test_clahe\n",
    "    \n",
    "    # Step 3: Resize and Normalize\n",
    "    print(\"Step 3/8: Resizing and Normalizing...\")\n",
    "    X_test_resized = np.array([cv2.resize(img, target_size) for img in X_test_clahe])\n",
    "    X_test_normalized = X_test_resized.astype('float32') / 255.0\n",
    "    X_test_with_channel = X_test_normalized.reshape(-1, target_size[0], target_size[1], 1)\n",
    "    preprocessing_info['normalized'] = X_test_with_channel\n",
    "    \n",
    "    # Step 4: HOG Feature Extraction\n",
    "    print(\"Step 4/8: Extracting HOG Features...\")\n",
    "    X_test_hog = []\n",
    "    for img in tqdm(X_test_with_channel):\n",
    "        hog_features = hog(\n",
    "            img.squeeze(),\n",
    "            pixels_per_cell=(8, 8),\n",
    "            cells_per_block=(2, 2),\n",
    "            visualize=False\n",
    "        )\n",
    "        X_test_hog.append(hog_features)\n",
    "    \n",
    "    X_test_hog = np.array(X_test_hog)\n",
    "    preprocessing_info['hog_features'] = X_test_hog\n",
    "    print(f\"HOG features shape: {X_test_hog.shape}\")\n",
    "    \n",
    "    # Step 5: LBP Feature Extraction\n",
    "    print(\"Step 5/8: Extracting LBP Features...\")\n",
    "    x_test_lbp = []\n",
    "    for img in tqdm(X_test_with_channel):\n",
    "        lbp_features = lbp_grid_hist(img.squeeze(), P=8, R=1, grid=(8,8), method='uniform')\n",
    "        x_test_lbp.append(lbp_features)\n",
    "    x_test_lbp = np.array(x_test_lbp)\n",
    "    print(f\"LBP features shape: {x_test_lbp.shape}\")\n",
    "    preprocessing_info['lbp_features'] = x_test_lbp\n",
    "    \n",
    "    # Step 6: SIFT Feature Extraction\n",
    "    print(\"Step 6/8: Extracting SIFT Features...\")\n",
    "    x_test_sift = []\n",
    "    sift = cv2.SIFT_create()\n",
    "    for img in tqdm(X_test_with_channel):\n",
    "        keypoints, descriptors = sift.detectAndCompute((img.squeeze() * 255).astype('uint8'), None)\n",
    "        if descriptors is not None:\n",
    "            sift_feature = descriptors.flatten()\n",
    "            if sift_feature.shape[0] < 128 * 10:  # Pad to fixed size\n",
    "                sift_feature = np.pad(sift_feature, (0, 128 * 10 - sift_feature.shape[0]), 'constant')\n",
    "            else:\n",
    "                sift_feature = sift_feature[:128 * 10]  # Truncate if too long\n",
    "        else:\n",
    "            sift_feature = np.zeros(128 * 10)  # No keypoints found\n",
    "        x_test_sift.append(sift_feature)\n",
    "    \n",
    "    x_test_sift = np.array(x_test_sift)\n",
    "    print(f\"SIFT features shape: {x_test_sift.shape}\")\n",
    "    preprocessing_info['sift_features'] = x_test_sift\n",
    "    \n",
    "    # Step 7: Combine all features (HOG + LBP + SIFT)\n",
    "    print(\"Step 7/8: Combining all features...\")\n",
    "    X_test_combined = np.concatenate([X_test_hog, x_test_lbp, x_test_sift], axis=1)\n",
    "    print(f\"Combined features shape: {X_test_combined.shape}\")\n",
    "    preprocessing_info['combined'] = X_test_combined\n",
    "    \n",
    "    # Step 8: Standard Scaling (using fitted scaler)\n",
    "    print(\"Step 8/8: Applying Standard Scaling and PCA...\")\n",
    "    X_test_scaled = scaler.transform(X_test_combined)  # Use transform, not fit_transform\n",
    "    preprocessing_info['scaled'] = X_test_scaled\n",
    "    \n",
    "    # PCA (using fitted PCA)\n",
    "    X_test_pca = pca.transform(X_test_scaled)  # Use transform, not fit_transform\n",
    "    preprocessing_info['pca'] = X_test_pca\n",
    "    \n",
    "    print(f\"âœ… Test data preprocessing completed!\")\n",
    "    print(f\"Final processed shape: {X_test_pca.shape}\")\n",
    "    print(f\"Original test set size: {len(X_test)}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    return X_test_pca, preprocessing_info\n",
    "\n",
    "\n",
    "print(\"ðŸ“Š Processing test data...\")\n",
    "X_test_processed, test_preprocessing_info = preprocess_test_data(\n",
    "    X_test, \n",
    "    scaler=scaler,  # Use the fitted scaler from training\n",
    "    pca=pca,        # Use the fitted PCA from training\n",
    "    target_size=(64, 64)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33721c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "models = {\n",
    "    \"SVM (RBF)\": SVC(\n",
    "        kernel='rbf',\n",
    "        C=10,\n",
    "        gamma=0.01,\n",
    "        class_weight='balanced',\n",
    "        probability=True,\n",
    "        random_state=42\n",
    "    ),\n",
    "    \n",
    "    \"Random Forest\": RandomForestClassifier(\n",
    "        n_estimators=300,\n",
    "        max_depth=20,\n",
    "        min_samples_split=4,\n",
    "        min_samples_leaf=2,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ),\n",
    "    \n",
    "    \"Logistic Regression\": LogisticRegression(\n",
    "        solver='lbfgs',\n",
    "        max_iter=2000,\n",
    "        C=1.0,\n",
    "        multi_class='auto',\n",
    "        class_weight='balanced',\n",
    "        random_state=42\n",
    "    ),\n",
    "    \n",
    "    \"XGBoost\": XGBClassifier(\n",
    "        n_estimators=300,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=6,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        gamma=0.1,\n",
    "        reg_lambda=1.0,\n",
    "        objective='multi:softmax',\n",
    "        num_class=len(np.unique(y)),\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        verbosity=0\n",
    "    )\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bb3e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results = []\n",
    "\n",
    "for name, model in tqdm(models.items()):\n",
    "    print(f\"\\nðŸ”¹ Training {name} with HOG + LBP + SIFT features...\")\n",
    "    model.fit(X_train_pca, y)  # Use X_train_pca (combined HOG + LBP + SIFT)\n",
    "    \n",
    "    y_pred = model.predict(X_test_processed)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    print(f\"ðŸŽ¯ {name} Accuracy: {acc*100:.2f}%\")\n",
    "    \n",
    "    results.append((name, acc))\n",
    "    print(classification_report(y_test, y_pred, target_names=emotion_labels))\n",
    "\n",
    "# Print results summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING COMPLETED - Results Summary:\")\n",
    "print(\"=\"*60)\n",
    "for model_name, accuracy in results:\n",
    "    print(f\"{model_name:25s}: {accuracy*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc56c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model Evaluation & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e73d998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organize results into structured format for evaluation\n",
    "model_results = {}\n",
    "\n",
    "for (model_name, accuracy), model in zip(results, models.values()):\n",
    "    y_pred = model.predict(X_test_processed)\n",
    "    \n",
    "    model_results[model_name] = {\n",
    "        'model': model,\n",
    "        'accuracy': accuracy,\n",
    "        'predictions': y_pred,\n",
    "        'classification_report': classification_report(y_test, y_pred, target_names=emotion_labels, output_dict=True)\n",
    "    }\n",
    "\n",
    "print(\"âœ… Model results organized for detailed evaluation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548dce72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Model Accuracy Comparison Bar Chart\n",
    "def plot_model_comparison(model_results):\n",
    "    \"\"\"\n",
    "    Plot comprehensive model comparison\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    model_names = list(model_results.keys())\n",
    "    accuracies = [model_results[name]['accuracy'] for name in model_names]\n",
    "    \n",
    "    # 1. Accuracy Bar Chart\n",
    "    ax1 = axes[0, 0]\n",
    "    colors = ['#3498db', '#e74c3c', '#2ecc71', '#f39c12']\n",
    "    bars = ax1.bar(model_names, accuracies, color=colors, alpha=0.8, edgecolor='black', linewidth=1.5)\n",
    "    ax1.set_title('Model Accuracy Comparison', fontsize=14, fontweight='bold')\n",
    "    ax1.set_ylabel('Accuracy', fontsize=12)\n",
    "    ax1.set_ylim(0, 1)\n",
    "    ax1.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "    \n",
    "    # Add accuracy values on bars\n",
    "    for bar, acc in zip(bars, accuracies):\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f'{acc:.3f}\\n({acc*100:.1f}%)',\n",
    "                ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    # 2. Macro-averaged Metrics (Precision, Recall, F1)\n",
    "    ax2 = axes[0, 1]\n",
    "    metrics_data = []\n",
    "    for name in model_names:\n",
    "        report = model_results[name]['classification_report']\n",
    "        metrics_data.append([\n",
    "            report['macro avg']['precision'],\n",
    "            report['macro avg']['recall'],\n",
    "            report['macro avg']['f1-score']\n",
    "        ])\n",
    "    \n",
    "    metrics_data = np.array(metrics_data)\n",
    "    x = np.arange(len(model_names))\n",
    "    width = 0.25\n",
    "    \n",
    "    ax2.bar(x - width, metrics_data[:, 0], width, label='Precision', alpha=0.8, color='#3498db')\n",
    "    ax2.bar(x, metrics_data[:, 1], width, label='Recall', alpha=0.8, color='#e74c3c')\n",
    "    ax2.bar(x + width, metrics_data[:, 2], width, label='F1-Score', alpha=0.8, color='#2ecc71')\n",
    "    \n",
    "    ax2.set_xlabel('Models', fontsize=12)\n",
    "    ax2.set_ylabel('Score', fontsize=12)\n",
    "    ax2.set_title('Macro-Averaged Metrics Comparison', fontsize=14, fontweight='bold')\n",
    "    ax2.set_xticks(x)\n",
    "    ax2.set_xticklabels(model_names, rotation=15, ha='right')\n",
    "    ax2.legend(loc='lower right')\n",
    "    ax2.set_ylim(0, 1)\n",
    "    ax2.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "    \n",
    "    # 3. Per-Class F1-Score Heatmap\n",
    "    ax3 = axes[1, 0]\n",
    "    f1_scores = []\n",
    "    for name in model_names:\n",
    "        report = model_results[name]['classification_report']\n",
    "        class_f1 = [report[emotion]['f1-score'] for emotion in emotion_labels]\n",
    "        f1_scores.append(class_f1)\n",
    "    \n",
    "    f1_scores = np.array(f1_scores)\n",
    "    im = ax3.imshow(f1_scores, cmap='YlGnBu', aspect='auto', vmin=0, vmax=1)\n",
    "    ax3.set_xticks(np.arange(len(emotion_labels)))\n",
    "    ax3.set_yticks(np.arange(len(model_names)))\n",
    "    ax3.set_xticklabels(emotion_labels, rotation=45, ha='right')\n",
    "    ax3.set_yticklabels(model_names)\n",
    "    ax3.set_title('Per-Class F1-Score Heatmap', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Add text annotations\n",
    "    for i in range(len(model_names)):\n",
    "        for j in range(len(emotion_labels)):\n",
    "            text = ax3.text(j, i, f'{f1_scores[i, j]:.2f}',\n",
    "                           ha=\"center\", va=\"center\", color=\"black\", fontsize=9)\n",
    "    \n",
    "    plt.colorbar(im, ax=ax3, label='F1-Score')\n",
    "    \n",
    "    # 4. Model Performance Summary Table\n",
    "    ax4 = axes[1, 1]\n",
    "    ax4.axis('tight')\n",
    "    ax4.axis('off')\n",
    "    \n",
    "    summary_data = []\n",
    "    for name in model_names:\n",
    "        report = model_results[name]['classification_report']\n",
    "        summary_data.append([\n",
    "            name,\n",
    "            f\"{model_results[name]['accuracy']:.4f}\",\n",
    "            f\"{report['macro avg']['precision']:.4f}\",\n",
    "            f\"{report['macro avg']['recall']:.4f}\",\n",
    "            f\"{report['macro avg']['f1-score']:.4f}\"\n",
    "        ])\n",
    "    \n",
    "    table = ax4.table(cellText=summary_data,\n",
    "                     colLabels=['Model', 'Accuracy', 'Precision', 'Recall', 'F1-Score'],\n",
    "                     cellLoc='center',\n",
    "                     loc='center',\n",
    "                     colWidths=[0.25, 0.15, 0.15, 0.15, 0.15])\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(10)\n",
    "    table.scale(1, 2)\n",
    "    \n",
    "    # Style header\n",
    "    for i in range(5):\n",
    "        table[(0, i)].set_facecolor('#3498db')\n",
    "        table[(0, i)].set_text_props(weight='bold', color='white')\n",
    "    \n",
    "    # Style rows\n",
    "    for i in range(1, len(summary_data) + 1):\n",
    "        for j in range(5):\n",
    "            if i % 2 == 0:\n",
    "                table[(i, j)].set_facecolor('#ecf0f1')\n",
    "    \n",
    "    ax4.set_title('Performance Summary', fontsize=14, fontweight='bold', pad=20)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot comparison\n",
    "plot_model_comparison(model_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b2a3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Confusion Matrices for All Models\n",
    "def plot_confusion_matrices(model_results, emotion_labels):\n",
    "    \"\"\"\n",
    "    Plot confusion matrices for all models in a grid\n",
    "    \"\"\"\n",
    "    n_models = len(model_results)\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 14))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for idx, (model_name, result) in enumerate(model_results.items()):\n",
    "        cm = confusion_matrix(y_test, result['predictions'])\n",
    "        \n",
    "        # Normalize confusion matrix\n",
    "        cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        \n",
    "        ax = axes[idx]\n",
    "        im = ax.imshow(cm_normalized, interpolation='nearest', cmap='Blues', vmin=0, vmax=1)\n",
    "        ax.set_title(f'{model_name}\\nAccuracy: {result[\"accuracy\"]:.3f}', \n",
    "                    fontsize=12, fontweight='bold')\n",
    "        \n",
    "        # Add colorbar\n",
    "        cbar = plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "        cbar.set_label('Normalized Count', rotation=270, labelpad=20)\n",
    "        \n",
    "        # Set ticks\n",
    "        tick_marks = np.arange(len(emotion_labels))\n",
    "        ax.set_xticks(tick_marks)\n",
    "        ax.set_yticks(tick_marks)\n",
    "        ax.set_xticklabels(emotion_labels, rotation=45, ha='right')\n",
    "        ax.set_yticklabels(emotion_labels)\n",
    "        \n",
    "        # Add text annotations\n",
    "        thresh = cm_normalized.max() / 2.\n",
    "        for i in range(cm_normalized.shape[0]):\n",
    "            for j in range(cm_normalized.shape[1]):\n",
    "                ax.text(j, i, f'{cm[i, j]}\\n({cm_normalized[i, j]:.2f})',\n",
    "                       ha=\"center\", va=\"center\",\n",
    "                       color=\"white\" if cm_normalized[i, j] > thresh else \"black\",\n",
    "                       fontsize=8)\n",
    "        \n",
    "        ax.set_ylabel('True Label', fontsize=10)\n",
    "        ax.set_xlabel('Predicted Label', fontsize=10)\n",
    "        ax.grid(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle('Confusion Matrices for All Models', fontsize=16, fontweight='bold', y=1.00)\n",
    "    plt.show()\n",
    "\n",
    "# Plot confusion matrices\n",
    "plot_confusion_matrices(model_results, emotion_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bcfd57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Per-Class Performance Analysis\n",
    "def plot_per_class_analysis(model_results, emotion_labels):\n",
    "    \"\"\"\n",
    "    Detailed per-class performance analysis across all models\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
    "    \n",
    "    # Prepare data\n",
    "    model_names = list(model_results.keys())\n",
    "    \n",
    "    # 1. Per-Class Precision\n",
    "    ax1 = axes[0, 0]\n",
    "    precision_data = []\n",
    "    for name in model_names:\n",
    "        report = model_results[name]['classification_report']\n",
    "        class_precision = [report[emotion]['precision'] for emotion in emotion_labels]\n",
    "        precision_data.append(class_precision)\n",
    "    \n",
    "    x = np.arange(len(emotion_labels))\n",
    "    width = 0.2\n",
    "    for i, (name, data) in enumerate(zip(model_names, precision_data)):\n",
    "        ax1.bar(x + i * width, data, width, label=name, alpha=0.8)\n",
    "    \n",
    "    ax1.set_xlabel('Emotion Classes', fontsize=12)\n",
    "    ax1.set_ylabel('Precision', fontsize=12)\n",
    "    ax1.set_title('Per-Class Precision Comparison', fontsize=14, fontweight='bold')\n",
    "    ax1.set_xticks(x + width * 1.5)\n",
    "    ax1.set_xticklabels(emotion_labels, rotation=45, ha='right')\n",
    "    ax1.legend(loc='lower right', fontsize=9)\n",
    "    ax1.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "    ax1.set_ylim(0, 1)\n",
    "    \n",
    "    # 2. Per-Class Recall\n",
    "    ax2 = axes[0, 1]\n",
    "    recall_data = []\n",
    "    for name in model_names:\n",
    "        report = model_results[name]['classification_report']\n",
    "        class_recall = [report[emotion]['recall'] for emotion in emotion_labels]\n",
    "        recall_data.append(class_recall)\n",
    "    \n",
    "    for i, (name, data) in enumerate(zip(model_names, recall_data)):\n",
    "        ax2.bar(x + i * width, data, width, label=name, alpha=0.8)\n",
    "    \n",
    "    ax2.set_xlabel('Emotion Classes', fontsize=12)\n",
    "    ax2.set_ylabel('Recall', fontsize=12)\n",
    "    ax2.set_title('Per-Class Recall Comparison', fontsize=14, fontweight='bold')\n",
    "    ax2.set_xticks(x + width * 1.5)\n",
    "    ax2.set_xticklabels(emotion_labels, rotation=45, ha='right')\n",
    "    ax2.legend(loc='lower right', fontsize=9)\n",
    "    ax2.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "    ax2.set_ylim(0, 1)\n",
    "    \n",
    "    # 3. Per-Class F1-Score\n",
    "    ax3 = axes[1, 0]\n",
    "    f1_data = []\n",
    "    for name in model_names:\n",
    "        report = model_results[name]['classification_report']\n",
    "        class_f1 = [report[emotion]['f1-score'] for emotion in emotion_labels]\n",
    "        f1_data.append(class_f1)\n",
    "    \n",
    "    for i, (name, data) in enumerate(zip(model_names, f1_data)):\n",
    "        ax3.bar(x + i * width, data, width, label=name, alpha=0.8)\n",
    "    \n",
    "    ax3.set_xlabel('Emotion Classes', fontsize=12)\n",
    "    ax3.set_ylabel('F1-Score', fontsize=12)\n",
    "    ax3.set_title('Per-Class F1-Score Comparison', fontsize=14, fontweight='bold')\n",
    "    ax3.set_xticks(x + width * 1.5)\n",
    "    ax3.set_xticklabels(emotion_labels, rotation=45, ha='right')\n",
    "    ax3.legend(loc='lower right', fontsize=9)\n",
    "    ax3.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "    ax3.set_ylim(0, 1)\n",
    "    \n",
    "    # 4. Average Performance by Emotion (across all models)\n",
    "    ax4 = axes[1, 1]\n",
    "    avg_precision = np.mean(precision_data, axis=0)\n",
    "    avg_recall = np.mean(recall_data, axis=0)\n",
    "    avg_f1 = np.mean(f1_data, axis=0)\n",
    "    \n",
    "    x_pos = np.arange(len(emotion_labels))\n",
    "    width = 0.25\n",
    "    \n",
    "    ax4.bar(x_pos - width, avg_precision, width, label='Avg Precision', alpha=0.8, color='#3498db')\n",
    "    ax4.bar(x_pos, avg_recall, width, label='Avg Recall', alpha=0.8, color='#e74c3c')\n",
    "    ax4.bar(x_pos + width, avg_f1, width, label='Avg F1-Score', alpha=0.8, color='#2ecc71')\n",
    "    \n",
    "    ax4.set_xlabel('Emotion Classes', fontsize=12)\n",
    "    ax4.set_ylabel('Score', fontsize=12)\n",
    "    ax4.set_title('Average Performance by Emotion (All Models)', fontsize=14, fontweight='bold')\n",
    "    ax4.set_xticks(x_pos)\n",
    "    ax4.set_xticklabels(emotion_labels, rotation=45, ha='right')\n",
    "    ax4.legend(loc='lower right')\n",
    "    ax4.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "    ax4.set_ylim(0, 1)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, (p, r, f) in enumerate(zip(avg_precision, avg_recall, avg_f1)):\n",
    "        ax4.text(i - width, p + 0.01, f'{p:.2f}', ha='center', va='bottom', fontsize=8)\n",
    "        ax4.text(i, r + 0.01, f'{r:.2f}', ha='center', va='bottom', fontsize=8)\n",
    "        ax4.text(i + width, f + 0.01, f'{f:.2f}', ha='center', va='bottom', fontsize=8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot per-class analysis\n",
    "plot_per_class_analysis(model_results, emotion_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db7d936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Sample Predictions Visualization (Best Model)\n",
    "def visualize_predictions(model_results, X_test, y_test, emotion_labels, n_samples=12):\n",
    "    \"\"\"\n",
    "    Visualize sample predictions from the best model\n",
    "    \"\"\"\n",
    "    # Find best model\n",
    "    best_model_name = max(model_results.keys(), key=lambda k: model_results[k]['accuracy'])\n",
    "    best_predictions = model_results[best_model_name]['predictions']\n",
    "    \n",
    "    # Select samples: correct and incorrect predictions\n",
    "    correct_indices = np.where(best_predictions == y_test)[0]\n",
    "    incorrect_indices = np.where(best_predictions != y_test)[0]\n",
    "    \n",
    "    # Sample half correct, half incorrect\n",
    "    n_correct = n_samples // 2\n",
    "    n_incorrect = n_samples - n_correct\n",
    "    \n",
    "    if len(correct_indices) >= n_correct and len(incorrect_indices) >= n_incorrect:\n",
    "        selected_correct = np.random.choice(correct_indices, n_correct, replace=False)\n",
    "        selected_incorrect = np.random.choice(incorrect_indices, n_incorrect, replace=False)\n",
    "        selected_indices = np.concatenate([selected_correct, selected_incorrect])\n",
    "    else:\n",
    "        selected_indices = np.random.choice(len(X_test), n_samples, replace=False)\n",
    "    \n",
    "    # Plot\n",
    "    fig, axes = plt.subplots(3, 4, figsize=(16, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for idx, test_idx in enumerate(selected_indices):\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        # Get image\n",
    "        img = X_test[test_idx]\n",
    "        \n",
    "        # Handle different image formats\n",
    "        if img.max() <= 1.0:\n",
    "            display_img = img\n",
    "        else:\n",
    "            display_img = img / 255.0\n",
    "        \n",
    "        # Display image\n",
    "        ax.imshow(display_img, cmap='gray')\n",
    "        \n",
    "        # Get prediction and true label\n",
    "        true_label = emotion_labels[y_test[test_idx]]\n",
    "        pred_label = emotion_labels[best_predictions[test_idx]]\n",
    "        \n",
    "        # Color: green for correct, red for incorrect\n",
    "        is_correct = (y_test[test_idx] == best_predictions[test_idx])\n",
    "        color = 'green' if is_correct else 'red'\n",
    "        marker = 'âœ“' if is_correct else 'âœ—'\n",
    "        \n",
    "        ax.set_title(f'{marker} True: {true_label}\\nPred: {pred_label}',\n",
    "                    fontsize=10, color=color, fontweight='bold')\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.suptitle(f'Sample Predictions - Best Model: {best_model_name}\\n'\n",
    "                f'Accuracy: {model_results[best_model_name][\"accuracy\"]:.3f}',\n",
    "                fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize predictions\n",
    "visualize_predictions(model_results, X_test, y_test, emotion_labels, n_samples=12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0dd1a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Error Analysis - Most Confused Pairs\n",
    "def plot_error_analysis(model_results, emotion_labels):\n",
    "    \"\"\"\n",
    "    Analyze and visualize the most confused emotion pairs\n",
    "    \"\"\"\n",
    "    best_model_name = max(model_results.keys(), key=lambda k: model_results[k]['accuracy'])\n",
    "    best_predictions = model_results[best_model_name]['predictions']\n",
    "    \n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_test, best_predictions)\n",
    "    \n",
    "    # Find most confused pairs (excluding diagonal)\n",
    "    confused_pairs = []\n",
    "    for i in range(len(emotion_labels)):\n",
    "        for j in range(len(emotion_labels)):\n",
    "            if i != j:\n",
    "                confused_pairs.append((emotion_labels[i], emotion_labels[j], cm[i, j]))\n",
    "    \n",
    "    # Sort by confusion count\n",
    "    confused_pairs.sort(key=lambda x: x[2], reverse=True)\n",
    "    top_confusions = confused_pairs[:10]\n",
    "    \n",
    "    # Plot\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 6))\n",
    "    \n",
    "    # 1. Top 10 Confused Pairs\n",
    "    pair_labels = [f\"{pair[0]}\\nâ†’ {pair[1]}\" for pair in top_confusions]\n",
    "    counts = [pair[2] for pair in top_confusions]\n",
    "    \n",
    "    colors_gradient = plt.cm.Reds(np.linspace(0.4, 0.9, len(counts)))\n",
    "    bars = ax1.barh(range(len(pair_labels)), counts, color=colors_gradient, edgecolor='black')\n",
    "    ax1.set_yticks(range(len(pair_labels)))\n",
    "    ax1.set_yticklabels(pair_labels, fontsize=10)\n",
    "    ax1.set_xlabel('Number of Misclassifications', fontsize=12)\n",
    "    ax1.set_title(f'Top 10 Most Confused Emotion Pairs\\n({best_model_name})', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "    ax1.invert_yaxis()\n",
    "    ax1.grid(axis='x', alpha=0.3, linestyle='--')\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, (bar, count) in enumerate(zip(bars, counts)):\n",
    "        ax1.text(count + 1, i, str(int(count)), \n",
    "                va='center', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    # 2. Confusion Distribution Heatmap (off-diagonal only)\n",
    "    cm_off_diag = cm.copy()\n",
    "    np.fill_diagonal(cm_off_diag, 0)\n",
    "    \n",
    "    im = ax2.imshow(cm_off_diag, cmap='Reds', aspect='auto')\n",
    "    ax2.set_xticks(np.arange(len(emotion_labels)))\n",
    "    ax2.set_yticks(np.arange(len(emotion_labels)))\n",
    "    ax2.set_xticklabels(emotion_labels, rotation=45, ha='right')\n",
    "    ax2.set_yticklabels(emotion_labels)\n",
    "    ax2.set_xlabel('Predicted Label', fontsize=12)\n",
    "    ax2.set_ylabel('True Label', fontsize=12)\n",
    "    ax2.set_title('Misclassification Heatmap\\n(Diagonal Removed)', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Add text annotations\n",
    "    for i in range(len(emotion_labels)):\n",
    "        for j in range(len(emotion_labels)):\n",
    "            if i != j and cm_off_diag[i, j] > 0:\n",
    "                ax2.text(j, i, int(cm_off_diag[i, j]),\n",
    "                       ha=\"center\", va=\"center\", \n",
    "                       color=\"white\" if cm_off_diag[i, j] > cm_off_diag.max()/2 else \"black\",\n",
    "                       fontsize=9, fontweight='bold')\n",
    "    \n",
    "    plt.colorbar(im, ax=ax2, label='Misclassification Count')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print error analysis summary\n",
    "    print(\"=\"*60)\n",
    "    print(f\"ERROR ANALYSIS SUMMARY - {best_model_name}\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\\nTop 5 Most Confused Pairs:\")\n",
    "    for i, (true_label, pred_label, count) in enumerate(top_confusions[:5], 1):\n",
    "        percentage = (count / cm.sum()) * 100\n",
    "        print(f\"{i}. {true_label} â†’ {pred_label}: {int(count)} times ({percentage:.2f}% of all predictions)\")\n",
    "    \n",
    "    total_errors = cm.sum() - np.trace(cm)\n",
    "    print(f\"\\nTotal Misclassifications: {int(total_errors)} out of {int(cm.sum())} \"\n",
    "          f\"({(total_errors/cm.sum())*100:.2f}%)\")\n",
    "\n",
    "# Plot error analysis\n",
    "plot_error_analysis(model_results, emotion_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e98a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Final Summary Report\n",
    "def print_final_summary(model_results):\n",
    "    \"\"\"\n",
    "    Print comprehensive final summary report\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\" \" * 25 + \"FINAL EVALUATION REPORT\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Best model\n",
    "    best_model_name = max(model_results.keys(), key=lambda k: model_results[k]['accuracy'])\n",
    "    best_accuracy = model_results[best_model_name]['accuracy']\n",
    "    best_report = model_results[best_model_name]['classification_report']\n",
    "    \n",
    "    print(f\"\\nðŸ† BEST MODEL: {best_model_name}\")\n",
    "    print(f\"   Accuracy: {best_accuracy:.4f} ({best_accuracy*100:.2f}%)\")\n",
    "    print(f\"   Macro Precision: {best_report['macro avg']['precision']:.4f}\")\n",
    "    print(f\"   Macro Recall: {best_report['macro avg']['recall']:.4f}\")\n",
    "    print(f\"   Macro F1-Score: {best_report['macro avg']['f1-score']:.4f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"DETAILED MODEL COMPARISON:\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    # Sort models by accuracy\n",
    "    sorted_models = sorted(model_results.items(), key=lambda x: x[1]['accuracy'], reverse=True)\n",
    "    \n",
    "    for rank, (model_name, result) in enumerate(sorted_models, 1):\n",
    "        report = result['classification_report']\n",
    "        print(f\"\\n{rank}. {model_name}\")\n",
    "        print(f\"   Accuracy:  {result['accuracy']:.4f}\")\n",
    "        print(f\"   Precision: {report['macro avg']['precision']:.4f}\")\n",
    "        print(f\"   Recall:    {report['macro avg']['recall']:.4f}\")\n",
    "        print(f\"   F1-Score:  {report['macro avg']['f1-score']:.4f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"PER-EMOTION PERFORMANCE (Best Model):\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    for emotion in emotion_labels:\n",
    "        metrics = best_report[emotion]\n",
    "        print(f\"\\n{emotion.upper():12s} | \"\n",
    "              f\"Precision: {metrics['precision']:.3f} | \"\n",
    "              f\"Recall: {metrics['recall']:.3f} | \"\n",
    "              f\"F1: {metrics['f1-score']:.3f} | \"\n",
    "              f\"Support: {int(metrics['support'])}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"FEATURE EXTRACTION PIPELINE:\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"  1. Preprocessing: Median Filter â†’ CLAHE â†’ Resize (64Ã—64) â†’ Normalize\")\n",
    "    print(\"  2. Feature Extraction:\")\n",
    "    print(\"     - HOG (Histogram of Oriented Gradients)\")\n",
    "    print(\"     - LBP (Local Binary Patterns)\")\n",
    "    print(\"     - SIFT (Scale-Invariant Feature Transform)\")\n",
    "    print(\"  3. Dimensionality Reduction: StandardScaler â†’ PCA (95% variance)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    return best_model_name, best_accuracy\n",
    "\n",
    "# Print final summary\n",
    "best_model, best_acc = print_final_summary(model_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d337a480",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7dfcf6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
